{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS 2: FREQUENCY LISTS AND KEYNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will conduct exploratory analysis of the corpora and:\n",
    "\n",
    " - look at most common words and bigrams for both corpora (different years)\n",
    " - look at the most distinctive words to the two corpora \n",
    " - look at the type of words that come after thankful/grateful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2019 = json.load(open(\"../data/cleaned/tweets_2019.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2020 = json.load(open(\"../data/cleaned/tweets_2020.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at frequencies of words we need to put tweets into string and then tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make tweets into string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_2019 = ''.join(tweets_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_2020 = ''.join(tweets_2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_2019 = tokenize(string_2019, lowercase = True, strip_chars = '.,!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_2020 = tokenize(string_2020, lowercase = True, strip_chars = '.,!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: FREQUENCY LISTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at frequency lists for the two years so we can see how loved ones and health shows up so we know which words/phrases to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_2019 = Counter(tokens_2019)\n",
    "bigram_freq_2019 = Counter(get_bigram_tokens(tokens_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 words\n",
      "============\n",
      "[('for', 9222), ('and', 9026), ('to', 8736), ('the', 8247), ('you', 6574), ('i', 5684), ('a', 5402), ('of', 4837), ('thanksgiving', 4123), ('my', 4026), ('thankful', 3101), ('all', 3035), ('in', 2858), ('is', 2767), ('this', 2615), ('that', 2465), ('grateful', 2295), ('so', 2256), ('have', 2237), ('we', 2233), ('your', 2226), ('be', 2187), ('with', 2115), ('thanks', 2094), ('are', 1932), ('happy', 1926), ('our', 1758), ('me', 1740), ('family', 1710), ('day', 1700)]\n",
      "\n",
      "Top 30 bigrams\n",
      "==============\n",
      "[('thankful for', 2354), ('grateful for', 1503), ('happy thanksgiving', 1288), ('for the', 1093), ('to be', 889), ('for all', 827), ('i am', 827), ('thank you', 787), ('give thanks', 758), ('for my', 592), ('of the', 582), ('all the', 572), ('thanks for', 562), ('for you', 562), ('to you', 522), ('all of', 518), ('you and', 503), ('in the', 467), ('we are', 447), ('thanksgiving to', 445), ('have a', 436), ('my life', 428), ('thanks to', 425), ('so much', 418), ('and i', 413), ('to the', 412), ('in my', 390), ('you all', 380), ('of you', 376), ('i appreciate', 374)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 30 words\\n============')\n",
    "print(word_freq_2019.most_common(30)) \n",
    "\n",
    "print('\\nTop 30 bigrams\\n==============')\n",
    "print(bigram_freq_2019.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_2020 = Counter(tokens_2020)\n",
    "bigram_freq_2020 = Counter(get_bigram_tokens(tokens_2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 words\n",
      "============\n",
      "[('for', 8791), ('and', 8691), ('to', 8517), ('the', 8180), ('you', 7138), ('i', 5850), ('a', 5347), ('of', 4542), ('my', 3870), ('thanksgiving', 3599), ('all', 2978), ('thankful', 2817), ('this', 2811), ('is', 2747), ('in', 2708), ('that', 2441), ('so', 2409), ('grateful', 2367), ('we', 2328), ('your', 2270), ('have', 2253), ('be', 2141), ('thanks', 1994), ('with', 1916), ('happy', 1897), ('it', 1793), ('are', 1792), ('me', 1657), ('on', 1611), ('but', 1532)]\n",
      "\n",
      "Top 30 bigrams\n",
      "==============\n",
      "[('thankful for', 2085), ('grateful for', 1556), ('happy thanksgiving', 1249), ('for the', 1025), ('i am', 859), ('to be', 854), ('thank you', 797), ('for you', 750), ('#oreninyourarea #oreninyourarea', 726), ('for all', 698), ('you and', 673), ('give thanks', 649), ('to you', 621), ('of the', 602), ('this year', 556), ('for my', 529), ('all the', 528), ('thanks for', 511), ('so much', 482), ('all of', 450), ('you all', 449), ('we are', 449), ('in the', 439), ('have a', 429), ('and i', 409), ('thanksgiving to', 407), ('is a', 405), ('my life', 396), ('thanks to', 387), ('be thankful', 379)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 30 words\\n============')\n",
    "print(word_freq_2020.most_common(30)) \n",
    "\n",
    "print('\\nTop 30 bigrams\\n==============')\n",
    "print(bigram_freq_2020.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this tell us?\n",
    "\n",
    "1. We can already see how there is a lot of overlap in the most frequent words. \n",
    "\n",
    "  \n",
    "2. We see the subject of our hypothesis come up in these lists as \"family\", so we know that this is certainly a top topic in these expressions of gratitude. \n",
    "\n",
    "  \n",
    "3. We also see a lot of pronouns (I, we, you, me) and prepositional phrases (for my, of the, for you, to you) which will both be interesting to analyze in context. See later notebook on this once we test hypothesis.\n",
    "\n",
    "  \n",
    "4. However, overall, this is not very helpful as we cannot see what is most significant and different. \n",
    "\n",
    "  \n",
    "I'm going to take out stop words so that we can see which words are coming up most frequent and which words we can look at to test hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency lists without stop words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove stop words from these token lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_tokens_2019 = [word for word in tokens_2019 if (word not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_tokens_2020 = [word for word in tokens_2020 if (word not in stopwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_word_freq_2019 = Counter(no_stop_tokens_2019)\n",
    "no_stop_bigram_freq_2019 = Counter(get_bigram_tokens(no_stop_tokens_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 words\n",
      "============\n",
      "[('thanksgiving', 4123), ('thankful', 3101), ('grateful', 2295), ('thanks', 2094), ('happy', 1926), ('family', 1710), ('day', 1700), ('love', 1492), ('blessings', 1203), ('i’m', 1158), ('appreciate', 1107), ('give', 1028), ('&amp;', 1006), ('thank', 1001), ('friends', 984), ('life', 979), ('gratitude', 974), ('appreciation', 889), ('everyone', 844), ('people', 839), ('god', 838), ('today', 825), ('lucky', 799), ('much', 789), ('us', 788), ('thankfulness', 779), (\"i'm\", 745), ('year', 720), ('blessing', 708), ('hope', 677)]\n",
      "\n",
      "Top 30 bigrams\n",
      "==============\n",
      "[('happy thanksgiving', 1290), ('give thanks', 766), ('i’m thankful', 350), ('god bless', 323), ('family friends', 311), (\"i'm thankful\", 251), ('thanksgiving everyone', 226), ('i’m grateful', 218), ('many blessings', 191), ('every day', 168), ('thanksgiving day', 159), (\"i'm grateful\", 154), ('friends family', 142), ('thanksgiving family', 132), ('loved ones', 105), ('thanks lord', 100), ('thankful family', 98), ('express gratitude', 94), ('thanksgiving 🦃', 89), ('hope everyone', 87), ('really appreciate', 85), ('thankful friends', 83), ('show appreciation', 79), ('many things', 78), ('thank much', 74), ('much love', 73), ('day filled', 72), ('love endures', 69), ('1 chronicles', 69), ('thanksgiving grateful', 69)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 30 words\\n============')\n",
    "print(no_stop_word_freq_2019.most_common(30)) \n",
    "\n",
    "print('\\nTop 30 bigrams\\n==============')\n",
    "print(no_stop_bigram_freq_2019.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_word_freq_2020 = Counter(no_stop_tokens_2020)\n",
    "no_stop_bigram_freq_2020 = Counter(get_bigram_tokens(no_stop_tokens_2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 words\n",
      "============\n",
      "[('thanksgiving', 3599), ('thankful', 2817), ('grateful', 2367), ('thanks', 1994), ('happy', 1897), ('love', 1449), ('family', 1413), ('day', 1265), ('i’m', 1170), ('blessings', 1168), ('appreciate', 1151), ('&amp;', 1135), ('year', 1060), ('thank', 1014), ('gratitude', 989), ('#oreninyourarea', 924), ('appreciation', 917), ('god', 916), ('much', 893), ('give', 868), ('life', 838), ('people', 816), ('us', 800), ('everyone', 777), ('friends', 767), ('thankfulness', 763), ('today', 750), ('blessing', 715), ('one', 697), ('like', 696)]\n",
      "\n",
      "Top 30 bigrams\n",
      "==============\n",
      "[('happy thanksgiving', 1249), ('#oreninyourarea #oreninyourarea', 726), ('give thanks', 650), ('i’m thankful', 347), ('god bless', 346), ('i’m grateful', 270), ('thanksgiving everyone', 206), ('mau #oreninyourarea', 198), (\"i'm thankful\", 171), ('family friends', 168), ('many blessings', 145), (\"i'm grateful\", 142), ('#oreninyourarea mau', 132), ('every day', 113), ('loved ones', 112), ('thanksgiving day', 102), ('stay safe', 101), ('express gratitude', 99), ('friends family', 98), ('thanksgiving family', 95), ('really appreciate', 94), ('im thankful', 93), ('much thankful', 93), ('thankful family', 91), ('thank god', 86), ('much love', 80), ('count blessings', 76), ('appreciation post', 75), ('thankful friends', 74), ('thanks giving', 73)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 30 words\\n============')\n",
    "print(no_stop_word_freq_2020.most_common(30)) \n",
    "\n",
    "print('\\nTop 30 bigrams\\n==============')\n",
    "print(no_stop_bigram_freq_2020.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we find from these frequency lists without stop words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find words having to do with our analysis such as:\n",
    "- family \n",
    "- friends \n",
    "- people\n",
    "- everyone\n",
    "\n",
    "This is important for the testing of our hypothesis because we know that these topics we want to measure are prominent in the text -- now it is a matter of conducting analyses that can effectively compare the discussions of the topics of loved ones and health in the to corpora. \n",
    "\n",
    "We also find some interesting ideas related but not central to my hypothesis that we can test later:\n",
    "- pronouns: i, we, us\n",
    "- big picture nouns: love, life, today, day, god, hope\n",
    "- nouns referring to other people: people, everyone\n",
    "- verbs: give\n",
    "- modifiers/intensifiers: like, much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: KEYNESS\n",
    "\n",
    "#### We can also do a Keyness analysis to see what are the most distinct words between the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                     Corpus A Freq.Corpus B Freq.Keyness\n",
      "============================================================\n",
      "day                      1700      1265      54.754\n",
      "•                        130       41        46.723\n",
      "2019                     78        17        41.094\n",
      "cowboys                  73        19        32.638\n",
      "full                     224       120       29.672\n",
      "@                        266       158       25.468\n",
      "thanksgiving             4123      3599      24.898\n",
      "jones                    35        5         24.649\n",
      "chronicles               79        28        24.205\n",
      "loving                   100       41        24.171\n",
      "#family                  156       79        24.020\n",
      "wonderful                414       278       23.968\n",
      "🍁                        80        30        22.486\n",
      "friends                  984       767       22.358\n",
      "family                   1710      1413      22.076\n",
      "16:34                    71        26        20.711\n",
      "#food                    31        5         20.322\n",
      "troops                   33        7         17.781\n",
      "la                       62        23        17.726\n",
      "store                    35        8         17.698\n",
      "her                      350       242       17.488\n",
      "#gobblegobble            32        7         16.807\n",
      "season                   186       115       15.366\n",
      "ang                      25        5         14.115\n",
      "endureth                 47        17        13.970\n",
      "ate                      40        13        13.828\n",
      "😂                        61        26        13.721\n",
      "🦃                        213       141       13.191\n",
      "pic                      26        6         13.036\n",
      "ng                       28        7         13.030\n",
      "he                       558       434       12.909\n",
      "friday                   73        35        12.830\n",
      "practice                 68        32        12.468\n",
      "#love                    104       58        12.241\n",
      "#friends                 54        23        12.163\n",
      "#grateful                261       183       12.096\n",
      "night                    102       57        11.932\n",
      "amazing                  366       272       11.891\n",
      "tv                       32        10        11.635\n",
      "spend                    134       82        11.514\n",
      "unto                     56        25        11.494\n",
      "part                     149       94        11.365\n",
      "sa                       48        20        11.266\n",
      "grace                    63        30        11.247\n",
      "mercy                    80        42        11.206\n",
      "road                     24        6         11.169\n",
      "our                      1758      1530      11.164\n",
      "abundance                31        10        10.823\n",
      "course                   59        28        10.615\n",
      "relationship             21        5         10.235\n"
     ]
    }
   ],
   "source": [
    "calculate_keyness(word_freq_2019, word_freq_2020, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                     Corpus A Freq.Corpus B Freq.Keyness\n",
      "============================================================\n",
      "2020                     198       36        127.097\n",
      "safe                     331       120       107.418\n",
      "stay                     233       83        77.522\n",
      "year                     1060      720       73.116\n",
      "workers                  73        12        49.995\n",
      "you                      7138      6574      37.424\n",
      "through                  303       182       33.256\n",
      "dropped                  41        5         32.947\n",
      "but                      1532      1266      31.580\n",
      "check                    101       38        31.039\n",
      "courts                   50        10        30.006\n",
      "gates                    50        11        27.867\n",
      "im                       271       166       27.864\n",
      "u                        464       329       26.191\n",
      "click                    42        8         26.109\n",
      "💯                        41        8         25.053\n",
      "you’ve                   119       58        22.837\n",
      "album                    56        17        22.836\n",
      "different                113       54        22.632\n",
      "moots                    34        6         22.262\n",
      "sorry                    102       47        22.030\n",
      "healthy                  109       52        21.910\n",
      "difficult                58        19        21.574\n",
      "it                       1793      1567      20.647\n",
      "though                   124       65        20.063\n",
      "generations              36        8         19.900\n",
      "tweet                    164       97        18.914\n",
      "✨                        44        13        18.504\n",
      "lot                      284       196       18.239\n",
      "has                      706       568       18.200\n",
      "alone                    78        36        16.791\n",
      "been                     706       574       16.727\n",
      "his                      632       508       16.409\n",
      "as                       915       768       16.329\n",
      "🥺                        48        17        16.100\n",
      "tough                    54        21        15.771\n",
      "during                   140       84        15.421\n",
      "healthcare               25        5         15.003\n",
      "#thanksgivingday         56        23        14.959\n",
      "praise                   82        41        14.858\n",
      "crazy                    74        36        14.259\n",
      "stream                   47        18        14.058\n",
      "willing                  27        7         13.008\n",
      "heroes                   25        6         12.939\n",
      "#bethankful              34        11        12.846\n",
      "plate                    39        14        12.836\n",
      "congratulations          42        16        12.665\n",
      "health                   203       141       12.655\n",
      "retweet                  30        9         12.401\n",
      "looks                    68        34        12.321\n"
     ]
    }
   ],
   "source": [
    "calculate_keyness(word_freq_2020, word_freq_2019, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                     Corpus A Freq.Corpus B Freq.Keyness\n",
      "============================================================\n",
      "• •                      63        5         57.264\n",
      "he is                    196       90        37.920\n",
      "thanksgiving from        282       155       34.679\n",
      "family and               362       217       33.553\n",
      "for he                   125       50        31.563\n",
      "and friends              190       96        29.424\n",
      "more you                 49        10        27.233\n",
      "family to                129       58        26.091\n",
      "the cowboys              40        7         24.868\n",
      "with family              110       47        24.627\n",
      "1 chronicles             69        22        24.456\n",
      "black friday             47        11        23.269\n",
      "#thanksgiving #givethanks48        12        22.337\n",
      "full of                  141       73        20.503\n",
      "the art                  31        5         20.322\n",
      "may you                  76        30        19.633\n",
      "good; for                54        17        19.453\n",
      "on thanksgiving          153       84        18.879\n",
      "endureth for             46        13        18.840\n",
      "to yours                 146       80        18.120\n",
      "chronicles 16:34         67        26        17.812\n",
      "and every                125       66        17.240\n",
      "thanksgiving 🦃           89        41        17.099\n",
      "in your                  138       76        16.872\n",
      "yours happy              31        7         15.844\n",
      "have and                 46        15        15.833\n",
      "mercy endureth           42        13        15.455\n",
      "family friends           128       71        15.316\n",
      "thanks unto              52        19        15.220\n",
      "part of                  123       68        14.866\n",
      "thankful and             88        43        14.794\n",
      "of thankfulness          223       146       14.522\n",
      "#gratitude #thankful     35        10        14.160\n",
      "support of               27        6         13.992\n",
      "thankful for?            46        17        13.229\n",
      "not just                 54        22        13.202\n",
      "your day                 92        48        13.106\n",
      "our family               103       56        13.078\n",
      "for his                  120       69        12.824\n",
      "family that              33        10        12.463\n",
      "day with                 99        54        12.452\n",
      "turkey day               62        28        12.425\n",
      "blessed thanksgiving     48        19        12.341\n",
      "the more                 41        15        11.977\n",
      "your appreciation        37        13        11.482\n",
      "the lord;                24        6         11.169\n",
      "unto the                 43        17        11.079\n",
      "people that              61        29        10.931\n",
      "thankful for             2354      2085      10.884\n",
      "of blessings             41        16        10.798\n"
     ]
    }
   ],
   "source": [
    "calculate_keyness(bigram_freq_2019, bigram_freq_2020, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                     Corpus A Freq.Corpus B Freq.Keyness\n",
      "============================================================\n",
      "this year                556       291       90.310\n",
      "safe and                 131       30        70.618\n",
      "stay safe                101       19        63.337\n",
      "check out                70        13        44.289\n",
      "year but                 68        13        42.163\n",
      "lord is                  45        5         37.701\n",
      "year has                 57        10        37.465\n",
      "out my                   58        12        33.931\n",
      "for you                  750       562       31.370\n",
      "and his                  108       44        29.246\n",
      "you and                  673       503       28.582\n",
      "for u                    89        33        27.953\n",
      "his courts               48        10        27.932\n",
      "courts with              48        10        27.932\n",
      "gates with               49        11        26.860\n",
      "his name                 44        9         25.965\n",
      "know the                 50        12        25.877\n",
      "his gates                48        11        25.860\n",
      "you i                    119       55        25.545\n",
      "take care                38        7         24.177\n",
      "like i                   48        13        22.146\n",
      "has been                 216       135       20.704\n",
      "my new                   49        15        19.789\n",
      "into his                 39        10        18.990\n",
      "we still                 33        7         18.936\n",
      "a lot                    269       182       18.877\n",
      "through the              56        21        17.295\n",
      "looks like               27        5         17.117\n",
      "so glad                  29        6         16.965\n",
      "with thanksgiving        50        18        16.390\n",
      "im so                    44        15        15.544\n",
      "a safe                   67        30        15.311\n",
      "still have               42        14        15.280\n",
      "on all                   50        19        15.136\n",
      "with praise              31        8         15.002\n",
      "but we                   56        23        14.959\n",
      "you always               38        12        14.790\n",
      "you too                  136       83        14.159\n",
      "and people               26        6         13.925\n",
      "and safe                 38        13        13.362\n",
      "healthy and              31        9         13.292\n",
      "and healthy              27        7         13.008\n",
      "love you                 327       248       12.714\n",
      "and hope                 38        14        12.048\n",
      "appreciate each          22        5         11.936\n",
      "i’m grateful             200       141       11.612\n",
      "thanks a                 29        9         11.525\n",
      "comes to                 29        9         11.525\n",
      "if i                     84        47        11.432\n",
      "can be                   80        44        11.419\n"
     ]
    }
   ],
   "source": [
    "calculate_keyness(bigram_freq_2020, bigram_freq_2019, top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we find from keyness? We can already see our hypothesis tested in this analysis. \n",
    "\n",
    "Notable distinctive tokens/bigrams to 2019:\n",
    "\n",
    "- words we are interested in for our hypothesis: family, #family, friends\n",
    "- descriptions (adjectives/adverbs): full, loving, wonderful, amazing\n",
    "- nouns: day, night\n",
    "- big picture nouns: #love, gracy, mercy, abundance\n",
    "- pronouns: her, he, our\n",
    "\n",
    "Notable distinctive tokens/bigrams to 2020:\n",
    "\n",
    "- descriptions: safe, alone, tough, crazy, difficult, different, lot\n",
    "- modifiers: despite, though, but\n",
    "- verbs: stay \n",
    "- covid related nouns: workers, healthcare, heroes, health, healthy\n",
    "- referring to people: you (heroes, workers)\n",
    "\n",
    "- reflexive on this year: this year, stay safe\n",
    "\n",
    "\n",
    "#### What does this mean for our hypothesis?\n",
    "\n",
    "We can already see out hypothesis tested as family/friends is more distinctive in 2019 and health/healthy is more distinctive to 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Look for words around \"thankful\"\n",
    "\n",
    "We want to see other words/phrases that come up around this word to use in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KWIC for \"thankful\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_thankful_19 = []\n",
    "\n",
    "for tweet in tweets_2019:\n",
    "    tokens_19 = tokenize(tweet, lowercase = True)\n",
    "    kwic_19 = make_kwic(\"thankful\", tokens_19)\n",
    "    kwic_thankful_19.extend(kwic_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "thankful_19_sample = random.sample(kwic_thankful_19, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  thanksgiving to all be  thankful  and grateful for what\n",
      "                  i’ve received i’m also  thankful  for every bit of\n",
      "                       people i am truly  thankful  for but i aint\n",
      "                         be worse so i’m  thankful  for what i have\n",
      "                      what have you been  thankful  for this year blessed\n",
      "                                          thankful  for my beautiful girlfriend\n",
      "                   and i’m so incredibly  thankful  for all the wonderful\n",
      "                  extra thankful today 🥰  thankful  for friends amp family\n",
      "                                          thankful  for you and shawnabenson\n",
      "                                          thankful  for everyone whos tweeting\n",
      "                 i thankful this holiday  thankful  for all the food\n",
      "                             a lot to be  thankful  for i try to\n",
      "                       share what we are  thankful  for which casts a\n",
      "                     spirit every day im  thankful  for every one of\n",
      "                  lifesong family we are  thankful  for you thanksgiving happythanksgiving\n",
      "                                          thankful  for the fact that\n",
      "                                          thankful  for you guys ❤️❤️\n",
      "                        lot of people im  thankful  for their support and\n",
      "                        wife what she is  thankful  for thequeenspeaks nashville tn\n",
      "                                          thankful  for the handful of\n",
      "                                          thankful  for all the amazing\n",
      "                                          thankful  for the gainz i\n",
      "                                          thankful  for my family that\n",
      "                                          thankful  for my man💛 \n",
      "                                          thankful  for julian assange who\n",
      "                                          thankful  for getting a chance\n",
      "                                          thankful  for so many people\n",
      "                       today lol but i’m  thankful  for all the blessings\n",
      "                                          thankful  for our president trump\n",
      "                                          thankful  for everyone in life\n",
      "                                          thankful  for this ass whooping\n",
      "                      detroit pal we are  thankful  for every participant volunteer\n",
      "                                          thankful  for all the friends\n",
      "                                          thankful  for this great group\n",
      "                                          thankful  for a glimpse of\n",
      "                                          thankful  for my spicy pepper\n",
      "        heycooljeremy insanecandace i am  thankful  for being an energetic\n",
      "                                          thankful  for me bye \n",
      "                                          thankful  for i tell i\n",
      "                                          thankful  for the cowboys \n",
      "                          you have to be  thankful  for\" nvpeale happythanksgiving thanksgiving\n",
      "                                          thankful  grateful blessed httpstcoelwkgpuibt \n",
      "                       and every day i’m  thankful  he’s my president thankful\n",
      "                          that i am very  thankful  i hope everyone had\n",
      "        you can happythanksgiving family  thankful  thebiz thejourney bostonmarket httpstcofhyoiwxawn\n",
      "                           we have to be  thankful  to our friends and\n",
      "                          all of you and  thankful  to be a viking\n",
      "                     very blessed and so  thankful  today happy thanksgiving everyone\n",
      "                                          thankful  tonight for the roof\n",
      "                          and love 🥰 i’m  thankful  you’re here  \n"
     ]
    }
   ],
   "source": [
    "print_kwic(sort_kwic(thankful_19_sample , order=['R1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwic_thankful_20 = []\n",
    "\n",
    "for tweet in tweets_2020:\n",
    "    tokens_20 = tokenize(tweet, lowercase = True)\n",
    "    kwic_20 = make_kwic(\"thankful\", tokens_20)\n",
    "    kwic_thankful_20.extend(kwic_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "thankful_20_sample = random.sample(kwic_thankful_20, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     thanks we should be  thankful  all year long he\n",
      "                                          thankful  and grateful for every\n",
      "                   remember to always be  thankful  and count your blessings\n",
      "                                          thankful  but not complete \n",
      "                       woman can ask for  thankful  foe this beautiful princess\n",
      "                    of thankfulness i am  thankful  for you httpstcop7b8tba8hs \n",
      "                    my darkest times i’m  thankful  for harry for bringing\n",
      "                                          thankful  for y’all💜 httpstcout5yhnmlqn \n",
      "                  26 of thankfulness i’m  thankful  for my sister how\n",
      "          droakley1689 tomascol amen tom  thankful  for your steadfast faithfulness\n",
      "                                          thankful  for my family and\n",
      "                                          thankful  for everyone’s health and\n",
      "                     people smile i’m so  thankful  for you and what\n",
      "                                          thankful  for my family my\n",
      "                                          thankful  for our student and\n",
      "                                          thankful  for you both 🤍\n",
      "                        y2zoom and i are  thankful  for you stay blessed\n",
      "                      snatched away i am  thankful  for the very few\n",
      "                           so much to be  thankful  for httpstcoxwxenvdhjh  \n",
      "               happy thanksgiving ✨ very  thankful  for the new friends\n",
      "                                          thankful  for all of your\n",
      "                    \"im really sorry and  thankful  for the fans who\n",
      "                                          thankful  for living in such\n",
      "                        youtube i am not  thankful  for this garbage post\n",
      "                                          thankful  for tesla elonmusk for\n",
      "                        essay on why i’m  thankful  for them  \n",
      "                  thanksgiving what am i  thankful  for always knowing im\n",
      "  egirlkb zombiegirl317 jakmasta101 were  thankful  for you too bud\n",
      "                                          thankful  for my family with\n",
      "                                          thankful  for you💛  \n",
      "                                          thankful  for you all come\n",
      "                                          thankful  for hard dick 😘\n",
      "                           is much to be  thankful  for make time to\n",
      "                 employees what they are  thankful  for the responses are\n",
      "                                          thankful  for all the counties\n",
      "                                          thankful  for our student and\n",
      "                                          thankful  for all the clowntown\n",
      "                         huh lets see im  thankful  for my familys okay\n",
      "                                          thankful  for my family and\n",
      "                          miss him a lot  thankful  grateful for so many\n",
      "                      a safe healthy and  thankful  holiday 🦃 msustemfee thanksgiving\n",
      "                                          thankful  httpstconpyblrac9c   \n",
      "                                          thankful  recognizing the native lands\n",
      "                      this here bird app  thankful  that you all support\n",
      "                                          thankful  the greatest gift is\n",
      "                   courts with praise be  thankful  to him and bless\n",
      "           is available now thanksgiving  thankful  turkey turkeyday theitlistpodcast httpstcodnc18qsj4j\n",
      "                                          thankful  u r too \n",
      "                       waiting how to be  thankful  while you wait read\n",
      "              constant support lovely so  thankful  ❤️   \n"
     ]
    }
   ],
   "source": [
    "print_kwic(sort_kwic(thankful_20_sample, order=['R1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this tell us?\n",
    "\n",
    "1. We see that people are calling out others that are thankful for in several ways:\n",
    " -  \"you\" (as in directly talking about one/more people)\n",
    " -  \"my\" or \"our\" children/kid/friend (possessive)\n",
    " -  people/person/everyone (a group of people)\n",
    " \n",
    " \n",
    " **Once again, this tells us that these prounouns play a huge role. Stayed tuned on this.**\n",
    "\n",
    "2. It is difficult to tell who is actually being called out when we only have 4 words on either side. So we need to look at collocates around these words to see what is coming up.\n",
    "\n",
    "### What else can we do?\n",
    "\n",
    "We can use collocates around this word to see their contexts using token lists without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coll_2019 = collocates(no_stop_tokens_2019,\"thankful\", win=[6,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_19 = random.sample(coll_2019, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coll_2020 = collocates(no_stop_tokens_2020,\"thankful\", win=[6,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_20 = random.sample(coll_2020, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare these collocates in a **keyness analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_19_freq = Counter([word for word in tokens_2019 if (word in sample_19)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_20_freq = Counter([word for word in tokens_2020 if (word in sample_20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                     Corpus A Freq.Corpus B Freq.Keyness\n",
      "============================================================\n",
      "day                      1700      1265      37.886\n",
      "friends                  984       767       14.224\n",
      "family                   1710      1413      11.695\n",
      "thanksgiving             4123      3599      8.869\n",
      "#happythanksgivng        61        32        7.330\n"
     ]
    }
   ],
   "source": [
    "calculate_keyness(coll_19_freq, coll_20_freq, top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD                     Corpus A Freq.Corpus B Freq.Keyness\n",
      "============================================================\n",
      "year                     1060      720       90.536\n",
      "u                        464       329       33.197\n",
      "different                113       54        25.508\n",
      "lot                      284       196       22.767\n",
      "&amp;                    1135      1006      19.031\n",
      "much                     893       789       15.456\n",
      "help                     193       142       11.652\n",
      "grateful                 2367      2295      11.404\n",
      "like                     696       620       11.085\n",
      "thing                    188       143       9.581\n",
      "bless                    612       563       6.737\n"
     ]
    }
   ],
   "source": [
    "calculate_keyness(coll_20_freq, coll_19_freq, top=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this tell us?\n",
    "\n",
    "We are looking at what is distinctive about the words that come around the word \"thankful\". This should give us an indication of the topics being invoked around expressions of gratitude.\n",
    "\n",
    "We can see distinctive words to 2019:\n",
    " - reflexive on the day: day, thanksgiving\n",
    " - positive descriptions: wonderful, amazing\n",
    " - adding to hypothesis: friends, family \n",
    " \n",
    "Words distinctive to 2020:\n",
    " - reflexive on the year: year\n",
    " - pronound: im (i'm), u \n",
    " - adding to hypothesis: health\n",
    " - god\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on these three parts, what does this tell us about our hypothesis?\n",
    "\n",
    "Loved ones: We can see that direct mentions of loved ones (family and friends) are more distinctive to the 2019 corpus, in addition to prounouns (he, our, we). \n",
    "\n",
    "Health: We can see that mentions of health (health, healthy) are more distinctive to 2020. In addition to the word *safe* which makes sense given the pandemic.\n",
    "\n",
    "### Updated Hypothesis: It appears that loved ones appear more in 2019, and references to health more in 2020. \n",
    "\n",
    "### What do we need to do next?\n",
    "\n",
    "1. Look to each topic (loved ones and health) independently to measure and analyze how they function differently in the two corpora.\n",
    "\n",
    "2. Re-evaluate this hypothesis as we go along, possibly making it more nuanced as we discover both quantitatively and qualitatively how these topics are invoked in tweets.\n",
    "\n",
    "### NOTE: \n",
    "*We found other interesting patterns in these corpora that are not directly related to testing my hypothesis. Once I've explored my hypothesis further, we will go into these trends on pronouns and big picture words. See last analysis notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
