{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Setting up Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run functions in \"functions\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries to scrape tweets from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping for 30k tweets: 1000 tweets from queries of these 15 keywords/phrases for the day of Thanksgiving 2019 and 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. thanks\n",
    "2. give thanks\n",
    "3. thankful\n",
    "4. grateful \n",
    "5. gratitude\n",
    "6. blessed\n",
    "7. blessing\n",
    "8. blessings\n",
    "9. lucky\n",
    "10. thankful for \n",
    "11. grateful for\n",
    "12. appreciate\n",
    "13. appreciation\n",
    "14. thankfulness\n",
    "15. thanksgiving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data For 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thanks' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"thanks\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_1 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_1)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'give thanks' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"give thanks\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_2 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_2)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thankful' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"thankful\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_3 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_3)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'grateful' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"grateful\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_4 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_4)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'gratitude' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"gratitude\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_5 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_5)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'blessed' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"blessed\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_6 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_6)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'blessing' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"blessing\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_7 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_7)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'blessings' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"blessings\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_8 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_8)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'lucky' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"lucky\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_9 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_10)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thankful for' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"thankful for\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_10 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_10)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'grateful for' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"grateful for\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_11 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_11)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'appreciate' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"appreciate\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_12 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_12)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'appreciation' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"appreciation\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_13 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_13)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thankfulness' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"thankfulness\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_14 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_14)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thanksgiving' from 2019-11-28 to 2019-11-29 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2019'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2019-11-28\"\n",
    "until = \"2019-11-29\"\n",
    "queries = [\"thanksgiving\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_19_15 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_19_15)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_19_15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data For 2020:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thanks' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"thanks\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_1 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_1)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'give thanks' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"give thanks\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_2 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_2)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thankful' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"thankful\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_3 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_3)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'grateful' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"grateful\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_4 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_4)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'gratitude' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"gratitude\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_5 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_5)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'blessed' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"blessed\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_6 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_6)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'blessing' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"blessing\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_7 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_7)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'blessings' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"blessings\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_8 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_8)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'lucky' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"lucky\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_9 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_9)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thankful for' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"thankful for\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_10 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_10)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'grateful for' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"grateful for\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_11 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_11)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'appreciate' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"appreciate\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_12 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_12)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'appreciation' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"appreciation\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_13 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_13)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thankfulness' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"thankfulness\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_14 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_14)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets for query: 'thanksgiving' from 2020-11-26 to 2020-11-27 (max of 1000)\n",
      "\t retrieved 1000 tweets...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../data/raw/raw_2020'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "since = \"2020-11-26\"\n",
    "until = \"2020-11-27\"\n",
    "queries = [\"thanksgiving\"] \n",
    "\n",
    "for query in queries:\n",
    "    twitter_20_15 = download_query_tweets(query, since, until, 1000) \n",
    "    \n",
    "    outfilename = \"{}/{}_{}_to_{}.json\".format(DATA_DIR, query.replace(' ','_'), since, until)\n",
    "    \n",
    "    print('\\t retrieved {} tweets...\\n'.format(len(twitter_20_15)))\n",
    "    with open(outfilename,'w') as out:\n",
    "        out.write(json.dumps(twitter_20_15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Directives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have 15 json files in each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('../data/raw/raw_2019'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('../data/raw/raw_2020'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Tweets Into One List For 2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2019=[]\n",
    "\n",
    "for tweet in twitter_19_1:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_2:\n",
    "    tweets_2019.append(tweet['text']) \n",
    "for tweet in twitter_19_3:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_4:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_5:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_6:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_7:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_8:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_9:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_10:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_11:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_12:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_13:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_14:\n",
    "    tweets_2019.append(tweet['text'])\n",
    "for tweet in twitter_19_15:\n",
    "    tweets_2019.append(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save out as list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cleaned/tweets_2019.json\", \"w\") as out: \n",
    "    out.write(json.dumps(tweets_2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Tweets Into One List For 2020:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2020=[]\n",
    "\n",
    "for tweet in twitter_20_1:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_2:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_3:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_4:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_5:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_6:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_7:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_8:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_9:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_10:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_11:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_12:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_13:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_14:\n",
    "    tweets_2020.append(tweet['text'])\n",
    "for tweet in twitter_20_15:\n",
    "    tweets_2020.append(tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save out as list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cleaned/tweets_2020.json\", \"w\") as out: \n",
    "    out.write(json.dumps(tweets_2020))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check that we have the amount of tweets we want in each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_tweets = len(tweets_2020) + len(tweets_2019)\n",
    "total_num_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 30k tweets total to analyze."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
